---
title: "Upload App Sources"
openapi: "POST /upload/upload_app_sources"
description: "Specifically built to handle knowledge from workplace apps. Parsing, chunking, and indexing depends on the kind of app (eg - gmail, slack, documents, etc). "
---

import { TableOfContents } from '/snippets/table-of-contents.jsx'

<Panel>
  <TableOfContents />
</Panel>

### Sample
``` bash expandable
curl -X 'POST' \
  'https://api.usecortex.ai/ingestion/upload-app' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "app_knowledge": [
    {
      "id": "string",
      "tenant_id": "string",
      "sub_tenant_id": "string",
      "title": "",
      "source": "",
      "description": "",
      "url": "",
      "timestamp": "",
      "content": {
        "text": "",
        "html_base64": "",
        "csv_base64": "",
        "markdown": "",
        "files": [
          {
            "additionalProp1": {}
          }
        ],
        "layout": [
          {
            "additionalProp1": {}
          }
        ]
      },
      "metadata": {
        "additionalProp1": {}
      },
      "additional_metadata": {
        "additionalProp1": {}
      },
      "relations": null
    }
  ],
  "upsert": true
}'

```

### SDK Examples

<Tabs>
  <Tab title="TypeScript">
    ```ts expandable
    const result = await client.upload.uploadAppSources({
      app_knowledge: [
        {
          id: "user-guide-1",
          tenant_id: "tenant_1234",
          sub_tenant_id: "sub_tenant_4567",
          title: "Feature X Guide",
          source: "notion",
          content: { text: "How to use feature X" },
          metadata: { dept: "engineering" },
          additional_metadata: { source: "database" }
        },
        {
          id: "user-guide-2", 
          tenant_id: "tenant_1234",
          sub_tenant_id: "sub_tenant_4567",
          title: "Feature Y Guide",
          source: "slack",
          content: { text: "How to use feature Y" },
          metadata: { dept: "engineering" },
          additional_metadata: { source: "api" }
        }
      ],
      upsert: true
    });
    ```

  </Tab>
  <Tab title="Python (Sync)">
    ```python expandable
    # Async usage is similar, just use async_client and await
    result = client.upload.upload_app_sources(
        app_knowledge=[
            {
                "id": "user-guide-1",
                "tenant_id": "tenant_1234",
                "sub_tenant_id": "sub_tenant_4567",
                "title": "Feature X Guide", 
                "source": "notion",
                "content": {"text": "How to use feature X"},
                "metadata": {"dept": "engineering"},
                "additional_metadata": {"source": "database"}
            },
            {
                "id": "user-guide-2",
                "tenant_id": "tenant_1234",
                "sub_tenant_id": "sub_tenant_4567",
                "title": "Feature Y Guide",
                "source": "slack",
                "content": {"text": "How to use feature Y"}, 
                "metadata": {"dept": "engineering"},
                "additional_metadata": {"source": "api"}
            }
        ],
        upsert=True
    )
    ```

  </Tab>
</Tabs>

Works similar to the upload knowledge endpoint but is specifically designed to upload multiple app sources (e.g., Gmail, Slack, Notion) in a single request using the standardized AppKnowledgeModel format. Each app upload is handled using specialized pipelines inside Cortex and can include various content types with rich metadata.

## AppKnowledgeModel Structure

The endpoint accepts an array of `AppKnowledgeModel` objects with the following structure:

### Required Fields
- **`id`** (string): Stable, unique identifier for the source
- **`tenant_id`** (string): Unique identifier for the tenant
- **`sub_tenant_id`** (string): Unique identifier for the sub-tenant

### Optional Fields
- **`title`** (string): Short human-readable title for the source
- **`source`** (string): Source of the knowledge (e.g., slack, gmail, notion)
- **`description`** (string): Long-form description providing additional context
- **`url`** (string): Canonical URL or reference link associated with the source
- **`timestamp`** (string): Creation or last-updated timestamp in ISO-8601 format
- **`content`** (ContentModel): Primary content payload for indexing and retrieval
- **`metadata`** (dict): Tenant-level metadata for organizing and filtering
- **`additional_metadata`** (dict): Document-specific metadata
- **`relations`** (ForcefulRelationsPayload | null): Controls relationship extraction

## Supported Apps

The following apps are currently supported for app source uploads:

**File Storage & Cloud Services:**
- `drive` - Google Drive
- `dropbox` - Dropbox Business
- `dropboxpersonal` - Dropbox Personal
- `onedrive` - Microsoft OneDrive
- `sharepoint` - Microsoft SharePoint

**CRM & Sales:**
- `intercom` - Intercom
- `salesforce` - Salesforce
- `hubspot` - HubSpot

**Communication & Collaboration:**
- `msteams` - Microsoft Teams
- `gmail` - Gmail
- `slack` - Slack
- `outlook` - Microsoft Outlook

**Project Management:**
- `jira` - Atlassian Jira
- `confluence` - Atlassian Confluence
- `shortcut` - Shortcut
- `linear` - Linear
- `asana` - Asana

**Productivity & Organization:**
- `notion` - Notion
- `googlecalendar` - Google Calendar

## App Source Processing Pipeline

When you upload app sources, each source goes through specialized processing pipelines tailored to the specific app type:

### 1. **Immediate Upload & App Detection**
- All app sources are immediately accepted and stored securely
- App type is automatically detected (Gmail, Slack, Notion, etc.)
- Each source is routed to its specialized processing pipeline
- You receive a confirmation response with individual `file_id`s for tracking

### 2. **App-Specific Processing Phase**
Each app source is processed using specialized pipelines:
- **Gmail**: Email parsing, thread reconstruction, attachment handling
- **Slack**: Message threading, channel context, user mentions
- **Notion**: Page hierarchy, block structure, database relationships
- **Documents**: Format-specific parsing (PDF, DOCX, etc.)
- **Custom Apps**: Configurable parsing based on app metadata

### 3. **Content Extraction & Normalization**
- **Multi-format Support**: Text, HTML, CSV, Markdown, and file attachments
- **Context Preservation**: Maintaining app-specific context and relationships
- **Metadata Enrichment**: Extracting app-specific metadata and timestamps
- **Content Cleaning**: Normalizing content while preserving structure

### 4. **Intelligent Chunking**
- App-aware chunking strategies preserve context and relationships
- Thread-based chunking for Gmail and Slack conversations
- Hierarchical chunking for Notion pages and databases
- Metadata is preserved and associated with each chunk

### 5. **Embedding Generation**
- Each chunk is converted into high-dimensional vector embeddings
- Embeddings capture semantic meaning and app-specific context
- Vectors are optimized for similarity search and retrieval
- Cross-app relationship embeddings for related content

### 6. **Indexing & Database Updates**
- Embeddings are stored in our vector database for fast similarity search
- Full-text search indexes are created for keyword-based queries
- App-specific metadata is indexed for filtering and faceted search
- Cross-references are established between related app sources

### 7. **Quality Assurance**
- App-specific quality checks ensure processing accuracy
- Content validation verifies extracted text completeness
- Relationship validation ensures proper context preservation
- Embedding quality is assessed for optimal retrieval performance

<Note>
**Processing Time**: App sources are processed in parallel using specialized pipelines. Most sources are fully processed and searchable within 2-5 minutes. Complex sources with multiple attachments may take up to 10 minutes. You can check processing status using the individual document IDs returned in the response.
</Note>

> **Recommended**: For optimal performance, limit each batch to a maximum of **20 app sources** per request. Send multiple batch requests with an interval of **1 second** between each request.

> **ID Management**: When you provide an `id` in the AppKnowledgeModel, that specific ID will be used to identify your content. The `id` field is required for all knowledge sources. This allows you to maintain consistent references to your content across your application while ensuring every piece of content has a unique identifier.

### `Upsert` parameter (optional)

For app sources, the identifier is `id`. When that identifier already exists, `upsert` (boolean, default `true`) controls the behavior:

| Value | Behavior |
|-------|----------|
| `true` | Replace existing (default) |
| `false` | Fail; do not overwrite |

Overwriting permanently removes and replaces existing chunks, embeddings, and indexes for that identifier.

**Example success response when overwriting by `id`:**


```json
{
  "message": "App sources uploaded successfully. Sources with existing ids have been overwritten.",
  "processed": 4,
  "failed": 0,
  "ids": ["gmail_123456", "slack_789012", "notion_345678", "drive_901234"],
  "overwritten_ids": ["gmail_123456", "slack_789012"],
  "status": "success"
}
```


## Content Model Structure

The `content` field in AppKnowledgeModel allows you to include various types of content alongside your main app source. It supports multiple content formats and can contain nested structures for complex documents.

### Content Object Structure

```json
{
  "content": {
    "text": "Plain text content",
    "html_base64": "base64_encoded_html",
    "csv_base64": "base64_encoded_csv",
    "markdown": "# Markdown content",
    "files": [{"name": "file.pdf", "data": "base64_data"}],
    "layout": [{"type": "section", "content": "..."}]
  }
}
```


### When to Use Each Content Field

**Core Content Fields:**
- `content.text`: Use for plain text content. Best for simple text documents, notes, or extracted text from other formats.
- `content.html_base64`: Use for HTML content encoded in base64. Ideal for web pages, rich text documents, or formatted content that needs to preserve HTML structure.
- `content.csv_base64`: Use for CSV data encoded in base64. Perfect for tabular data, spreadsheets, or structured data exports.
- `content.markdown`: Use for Markdown-formatted content. Great for documentation, README files, or any content that uses Markdown syntax.
- `content.files`: Use for binary file attachments as an array of file objects. Each file object should contain at least a `name` and `data` field (base64 encoded).
- `content.layout`: Use for structured document layouts as an array of layout objects. Useful for complex documents with sections, headers, or custom formatting.

### Content Format Guidelines

**For Text Content:**

```json
{
  "content": {
    "text": "This is plain text content that will be processed and indexed."
  }
}
```

**For HTML Content:**

```json
{
  "content": {
    "html_base64": "PGgxPkhlbGxvIFdvcmxkPC9oMT4="
  }
}
```

**For CSV Data:**

```json
{
  "content": {
    "csv_base64": "TmFtZSxBbW91bnQKSm9obiwxMDAKSmFuZSwyMDA="
  }
}
```

**For Markdown:**

```json
{
  "content": {
    "markdown": "# Document Title\n\nThis is **markdown** content with formatting."
  }
}
```

**For File Attachments:**

```json
{
  "content": {
    "files": [
      {
        "name": "document.pdf",
        "data": "JVBERi0xLjQKJcfsj6IKNSAwIG9iago8PAovVHlwZSAvUGFnZQovUGFyZW50IDMgMCBSCi9NZWRpYUJveCBbMCAwIDU5NSA4NDJdCi9SZXNvdXJjZXMgPDwKL0ZvbnQgPDwKL0YxIDIgMCBSCj4+Cj4+Ci9Db250ZW50cyA0IDAgUgo+PgplbmRvYmoK..."
      }
    ]
  }
}
```


### Best Practices

1. **Choose the Right Format**: Use the content field that best matches your data type for optimal processing.
2. **Base64 Encoding**: Always encode binary data (HTML, CSV, files) in base64 format.
3. **Content Size Limits**: Keep individual content items under 10MB for optimal processing performance.
4. **Metadata Usage**: Use the `metadata` and `additional_metadata` fields to store app-specific information that might be useful for filtering or organization.
5. **Source Specification**: Always specify the `source` field to ensure proper processing pipeline selection.
6. **Required Fields**: Ensure `id`, `tenant_id`, and `sub_tenant_id` are provided for all knowledge sources.

## Error Responses

All endpoints return consistent error responses following the standard format. For detailed error information, see our [Error Responses](/api-reference/error-responses) documentation.

