---
title: 'Add Memory'
openapi: 'POST /memories/add_memory'
---

import { TableOfContents } from '/snippets/table-of-contents.jsx'

<Panel>
  <TableOfContents />
</Panel>

### Sample Request

```bash
curl -X 'POST' \
  'https://api.usecortex.ai/memories/add_memory' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer <token>' \
  -d '{
  "memories": [
    {
      "text": "User prefers dark mode and detailed explanations",
      "infer": true,
      "user_name": "John"
    }
  ],
  "tenant_id": "tenant-01",
  "sub_tenant_id": "",
  "upsert": true
}'
```

### Examples
<Tabs>
  <Tab title="API Request">
    ```bash expandable
    # Simple text memory (no inference)
    curl --request POST \
      --url https://api.usecortex.ai/memories/add_memory \
      --header 'Authorization: Bearer <token>' \
      --header 'Content-Type: application/json' \
      --data '{
      "memories": [
        {
          "text": "Company policy document v2.0 - All employees must...",
          "infer": false,
          "title": "Company Policy"
        }
      ],
      "tenant_id": "tenant-01",
      "sub_tenant_id": "",
      "upsert": true
    }'
    
    # Text with inference enabled
    curl --request POST \
      --url https://api.usecortex.ai/memories/add_memory \
      --header 'Authorization: Bearer <token>' \
      --header 'Content-Type: application/json' \
      --data '{
      "memories": [
        {
          "text": "User wakes up early and enjoys jogging before work",
          "infer": true,
          "user_name": "John",
          "custom_instructions": "Extract user preferences and habits"
        }
      ],
      "tenant_id": "tenant-01",
      "sub_tenant_id": "",
      "upsert": true
    }'
    
    # Markdown content
    curl --request POST \
      --url https://api.usecortex.ai/memories/add_memory \
      --header 'Authorization: Bearer <token>' \
      --header 'Content-Type: application/json' \
      --data '{
      "memories": [
        {
          "text": "# Meeting Notes\n\n## Attendees\n- John\n- Jane\n\n## Action Items\n1. Review Q3 budget\n2. Prepare presentation",
          "is_markdown": true,
          "infer": false,
          "title": "Q3 Planning Meeting"
        }
      ],
      "tenant_id": "tenant-01",
      "sub_tenant_id": "",
      "upsert": true
    }'
    
    # User-assistant conversation pairs
    curl --request POST \
      --url https://api.usecortex.ai/memories/add_memory \
      --header 'Authorization: Bearer <token>' \
      --header 'Content-Type: application/json' \
      --data '{
      "memories": [
        {
          "user_assistant_pairs": [
            {"user": "What is my favorite color?", "assistant": "Based on our previous conversations, you prefer blue."},
            {"user": "Remember that I like dark mode", "assistant": "Noted! I will remember that you prefer dark mode interfaces."}
          ],
          "infer": true,
          "user_name": "John"
        }
      ],
      "tenant_id": "tenant-01",
      "sub_tenant_id": "",
      "upsert": true
    }'
    
    # Batch upload - multiple items with mixed settings
    curl --request POST \
      --url https://api.usecortex.ai/memories/add_memory \
      --header 'Authorization: Bearer <token>' \
      --header 'Content-Type: application/json' \
      --data '{
      "memories": [
        {
          "text": "User prefers detailed explanations",
          "infer": true,
          "user_name": "John"
        },
        {
          "text": "Company policy document - version 2.0",
          "infer": false,
          "title": "Policy Doc"
        },
        {
          "user_assistant_pairs": [
            {"user": "What are my settings?", "assistant": "You have dark mode enabled."}
          ],
          "infer": true,
          "user_name": "John"
        }
      ],
      "tenant_id": "tenant-01",
      "sub_tenant_id": "",
      "upsert": true
    }'
    ```

  </Tab>
  <Tab title="TypeScript">
    ```ts expandable
    // Simple text memory
    const result = await client.memories.addMemory({
      memories: [
        {
          text: "User prefers detailed explanations and dark mode",
          infer: true,
          user_name: "John"
        }
      ],
      tenant_id: "tenant-01",
      sub_tenant_id: "",
      upsert: true
    });
    
    // Markdown content
    const markdownResult = await client.memories.addMemory({
      memories: [
        {
          text: "# Meeting Notes\n\n## Key Points\n- Budget approved\n- Launch date: Q2",
          is_markdown: true,
          infer: false,
          title: "Meeting Notes"
        }
      ],
      tenant_id: "tenant-01",
      sub_tenant_id: "",
      upsert: true
    });
    
    // User-assistant pairs with inference
    const conversationResult = await client.memories.addMemory({
      memories: [
        {
          user_assistant_pairs: [
            { user: "What are my preferences?", assistant: "You prefer dark mode and detailed explanations." },
            { user: "How do I like my reports?", assistant: "You prefer weekly summary reports with charts." }
          ],
          infer: true,
          user_name: "John",
          custom_instructions: "Extract user preferences"
        }
      ],
      tenant_id: "tenant-01",
      sub_tenant_id: "",
      upsert: true
    });
    ```

  </Tab>
  <Tab title="Python (Sync)">
    ```python expandable
    from cortex import CortexClient
    
    client = CortexClient(api_key="your_api_key")
    
    # Simple text memory
    result = client.memories.add_memory(
        memories=[
            {
                "text": "User prefers detailed explanations and dark mode",
                "infer": True,
                "user_name": "John"
            }
        ],
        tenant_id="tenant-01",
        sub_tenant_id="",
        upsert=True
    )
    
    # Markdown content
    markdown_result = client.memories.add_memory(
        memories=[
            {
                "text": "# Meeting Notes\n\n## Key Points\n- Budget approved",
                "is_markdown": True,
                "infer": False,
                "title": "Meeting Notes"
            }
        ],
        tenant_id="tenant-01",
        sub_tenant_id="",
        upsert=True
    )
    
    # User-assistant pairs with inference
    conversation_result = client.memories.add_memory(
        memories=[
            {
                "user_assistant_pairs": [
                    {"user": "What are my preferences?", "assistant": "You prefer dark mode."},
                    {"user": "How do I like reports?", "assistant": "Weekly summaries with charts."}
                ],
                "infer": True,
                "user_name": "John",
                "custom_instructions": "Extract user preferences"
            }
        ],
        tenant_id="tenant-01",
        sub_tenant_id="",
        upsert=True
    )
    ```

  </Tab>
</Tabs>

## Overview

The **Add Memory** API is a unified endpoint for ingesting various types of content into Cortex. It combines the functionality of content upload and user memory storage into a single, flexible API.

## Content Types

The API supports three types of content input:

### 1. Raw Text (`text`)
Plain text content that will be chunked, embedded, and indexed for retrieval.


```json
{
  "text": "Your text content here",
  "infer": false
}
```


### 2. Markdown Content (`text` + `is_markdown`)
Markdown-formatted documents that preserve structure during processing.


```json
{
  "text": "# Title\n\n## Section\nContent here...",
  "is_markdown": true,
  "infer": false
}
```


### 3. User-Assistant Pairs (`user_assistant_pairs`)
Conversation exchanges between users and AI assistants, ideal for storing chat history and extracting user preferences.


```json
{
  "user_assistant_pairs": [
    {"user": "Question", "assistant": "Answer"}
  ],
  "infer": true
}
```


## Key Parameters

### `infer` (boolean)

Controls whether Cortex extracts additional insights from the content:

| Value | Behavior |
|-------|----------|
| `false` | Content is chunked and indexed as-is (faster processing) |
| `true` | Content is analyzed to extract implicit information, preferences, and insights |

<Info>
**When to use `infer=true`:**
- Storing user conversations to extract preferences
- Processing feedback or reviews
- Extracting insights from unstructured text

**When to use `infer=false`:**
- Uploading documents, policies, or factual content
- Storing structured data that doesn't need interpretation
- Performance-critical bulk uploads
</Info>

### `Upsert` parameter (optional)

For memories, the identifier is `source_id`. When that identifier already exists, `upsert` (boolean, default `true`) controls the behavior:

| Value | Behavior |
|-------|----------|
| `true` | Replace existing (default) |
| `false` | Fail; do not overwrite |

Overwriting permanently removes and replaces existing chunks, embeddings, and indexes for that identifier.

### `source_id` (optional)

Provide your own unique identifier, or let Cortex auto-generate one.

### `expiry_time` (optional)

Time-to-live in seconds. Content will be automatically deleted after this duration.

## Memory Item Fields

Each item in the `memories` array can have the following fields:

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `text` | string | * | Raw text or markdown content |
| `user_assistant_pairs` | array | * | Array of conversation pairs |
| `is_markdown` | boolean | No | Whether text is markdown formatted |
| `infer` | boolean | No | Enable inference processing (default: false) |
| `custom_instructions` | string | No | Guide inference processing |
| `user_name` | string | No | User's name for personalization |
| `source_id` | string | No | Custom unique identifier |
| `title` | string | No | Display title for the memory |
| `expiry_time` | integer | No | TTL in seconds |
| `tenant_metadata` | string | No | JSON string of tenant-level metadata |
| `document_metadata` | string | No | JSON string of document-level metadata |

<Note>
**\*** Either `text` OR `user_assistant_pairs` must be provided, but not both.
</Note>

## Response Format


```json
{
  "success": true,
  "message": "Memories queued for ingestion successfully",
  "results": [
    {
      "source_id": "abc123-def456",
      "title": "My Document",
      "status": "queued",
      "infer": false,
      "error": null
    }
  ],
  "success_count": 1,
  "failed_count": 0
}
```


## Processing Status

After submitting content, use the [Verify Processing](/api-reference/endpoint/verify-processing) endpoint to check status:


```bash
curl -X POST \
  'https://api.usecortex.ai/ingestion/verify-processing?file_id=<source_id>&tenant_id=<tenant_id>' \
  -H 'Authorization: Bearer <token>'
```


### Status Values

| Status | Description |
|--------|-------------|
| `queued` | Content is waiting to be processed |
| `processing` | Content is being chunked and embedded |
| `graph_creation` | Knowledge graph is being built |
| `completed` | Content is fully indexed and searchable |
| `errored` | Processing failed (check error_code) |

## Best Practices

<Info>
**Batch Uploads**: You can include multiple items in a single request for efficient bulk ingestion. Each item can have different `infer` settings.
</Info>

<Warning>
**Content Validation**: Either `text` or `user_assistant_pairs` must be provided for each memory item, but not both simultaneously.
</Warning>

<Note>
**Processing Time**: 
- `infer=false`: Typically 1-3 minutes
- `infer=true`: May take longer due to LLM inference processing
- Large batches: Processing time scales with content volume
</Note>

## Migration from Previous APIs

This endpoint replaces and unifies:
- `/memories/add-memories` (deprecated)
- `/ingestion/upload-content` (deprecated)

### Key Changes

| Old API | New API |
|---------|---------|
| `raw_text` | `text` |
| `content` in body wrapper | `memories` array at top level |
| Separate endpoints | Single unified endpoint |

## Error Responses

All endpoints return consistent error responses following the standard format. For detailed error information, see our [Error Responses](/api-reference/error-responses) documentation.
