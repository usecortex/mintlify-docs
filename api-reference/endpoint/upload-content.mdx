---
title: 'Upload Content'
openapi: 'POST /ingestion/upload-content'
tag: 'DEPRECATED'
---

<Note>
⚠️ **Deprecated**: This endpoint has been superseded by the unified [Add Memory](/api-reference/endpoint/add-memories) API. Please migrate to `/memories/add_memory` for new integrations.
</Note>

import { TableOfContents } from '/snippets/table-of-contents.jsx'

<Panel>
  <TableOfContents />
</Panel>

## Migration Guide

### Old Request Format

```bash
curl -X 'POST' \
  'https://api.usecortex.ai/ingestion/upload-content' \
  -d '{
  "content": {
    "contents": [
      {
        "file_id": "doc-123",
        "content": "Your text content here",
        "is_markdown": false
      }
    ]
  },
  "tenant_id": "tenant-01",
  "sub_tenant_id": "",
  "upsert": true
}'
```


### New Request Format (Recommended)


```bash
curl -X 'POST' \
  'https://api.usecortex.ai/memories/add_memory' \
  -d '{
  "memories": [
    {
      "source_id": "doc-123",
      "text": "Your text content here",
      "is_markdown": false,
      "infer": false
    }
  ],
  "tenant_id": "tenant-01",
  "sub_tenant_id": "",
  "upsert": true
}'
```


### Key Changes

| Old Field | New Field | Notes |
|-----------|-----------|-------|
| `content.contents[].content` | `memories[].text` | Content field renamed |
| `content.contents[].file_id` | `memories[].source_id` | ID field renamed |
| Nested structure | Flat structure | `memories` array at top level |
| No inference option | `infer` field | New inference capability |

## New Features in Add Memory API

The new `/memories/add_memory` endpoint provides:

- **Unified ingestion**: Single endpoint for text, markdown, and conversations
- **Inference mode**: Extract insights and preferences from content
- **User-assistant pairs**: Store conversation history
- **Batch uploads**: Multiple items in a single request

See the [Add Memory documentation](/api-reference/endpoint/add-memories) for full details.

---

## Legacy Documentation

<Accordion title="Original Upload Content Documentation">

### Examples

<Tabs>
  <Tab title="API Request">

    ```bash
    curl -X 'POST' \
  'https://api.usecortex.ai/ingestion/upload-content' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "content": {
    "contents": [
      {
        "file_id": "string",
        "content": "# Introduction\n\nThis is the document content.",
        "is_markdown": false,
        "tenant_metadata": "",
        "document_metadata": "",
        "relations": false
      }
    ]
  },
  "tenant_id": "string",
  "sub_tenant_id": "",
  "upsert": true
}'
    ```

  </Tab>
  <Tab title="TypeScript">

    ```ts
    const result = await client.upload.uploadText({
      tenant_id: "tenant_1234",
      sub_tenant_id: "sub_tenant_4567",
      body: {
        content: "Your text content here",
        file_id: "text_doc_123456",
        tenant_metadata: {},
        document_metadata: {}
      }
    });
    ```

  </Tab>
  <Tab title="Python (Sync)">

    ```python
    result = client.upload.upload_text(
        tenant_id="tenant_1234",
        sub_tenant_id="sub_tenant_4567",
        content="Your text content here",
        file_id="text_doc_123456",
        tenant_metadata={},
        document_metadata={}
    )
    ```

  </Tab>
</Tabs>

Upload text content directly to your tenant's knowledge base. The text will be processed, chunked, and indexed for search and retrieval.

### `Upsert` parameter (optional)

For text content, the identifier is `file_id`. When that identifier already exists, `upsert` (boolean, default `true`) controls the behavior:

| Value | Behavior |
|-------|----------|
| `true` | Replace existing (default) |
| `false` | Fail; do not overwrite |

Overwriting permanently removes and replaces existing chunks, embeddings, and indexes for that identifier.

### Text Processing Pipeline

When you upload text content, it goes through a streamlined processing pipeline optimized for direct text input:

1. **Immediate Upload & Queue** - Content is accepted and queued for processing
2. **Text Processing Phase** - Content validation, format detection, text normalization
3. **Intelligent Chunking** - Text is split into semantically meaningful chunks
4. **Embedding Generation** - Each chunk is converted into vector embeddings
5. **Indexing & Database Updates** - Embeddings stored in vector database

</Accordion>

## Error Responses

All endpoints return consistent error responses following the standard format. For detailed error information, see our [Error Responses](/api-reference/error-responses) documentation.
